# Caltech-CS165-Foundations-of-machine-learning-and-statistical-inference

Solutions to the 4 assignments of the course CS165: Foundations of machine learning and statistical inference offered by Caltech, WI 2022-23

## Final-project

Caltech CS165 final: fully deterministic neural network with deterministic initialization and non-stochastic full-batch training    

In this work, we aim to provide a successful counterexample to prove that such stochasticity is NOT necessary for ML theory.
We apply a fully deterministic initialization scheme as well as non-stochastic full-batch training, which initializes the weights of networks with only zeros and ones and explicit regularization during model training. Our work proposes that random weights and SGD may be unnecessary for neural networks, and it is possible to train neural networks without any randomness while achieving state-of-the-art performance.  

This project is based on Jiawei Zhao and Jonas Geiping's previous publications and work:  
- https://arxiv.org/abs/2110.12661  
- https://arxiv.org/abs/2109.14119

## Course schedule

Lecture 1: Introduction, Probability

Lecture 2: Sufficient statistics

Lecture 3: Bayesian

Lecture 4: Neyman Pearson

Lecture 5: Sequential detection

Lecture 6: Estimation and UMVU

Lecture 7: Cramer Rao

Lecture 8: Midterm exam

Lecture 9: Spectral Methods: PCA/CCA, HMM

Lecture 10: Spectral Methods: Tensor methods, method of moments

Lecture 11: Optimization: Non-convex

Lecture 12: Optimization in deep learning: Adam, CGD, MAdam

Lecture 13: generalization theory

Lecture 14: generalization theory

Lecture 15: approximation theory

Lecture 16: operator learning

Lecture 17: operator learning 